{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Data Analysis\n",
    "\n",
    "## 0. Install Required Libraries\n",
    "\n",
    "First, let's install all the required libraries for our analysis\n",
    "\n",
    "**Note**: This notebook is configured for Google Colab:\n",
    "1. It will first install required packages\n",
    "2. You can either:\n",
    "   - Upload the loan_data.csv directly when prompted\n",
    "   - Or mount your Google Drive if the file is stored there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "# Import and configure libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    !pip install pandas numpy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    # Configure plotting settings\n",
    "    plt.rcParams['figure.figsize'] = [10, 6]\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "except ImportError:\n",
    "    !pip install matplotlib seaborn\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.rcParams['figure.figsize'] = [10, 6]\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "\n",
    "try:\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "except ImportError:\n",
    "    !pip install scikit-learn\n",
    "    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "print(\"All required libraries are installed and imported successfully!\")\n",
    "\n",
    "# Try to use Google Colab for file upload\n",
    "try:\n",
    "    from google.colab import drive, files\n",
    "    print(\"Running in Google Colab environment\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Upload the data file\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Load the uploaded data\n",
    "    import io\n",
    "    df = pd.read_csv(io.BytesIO(uploaded['loan_data.csv']))\n",
    "except ImportError:\n",
    "    print(\"Running in local environment\")\n",
    "    # Load data locally\n",
    "    df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "### 2.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df[df.columns[-1]].value_counts().plot(kind='bar')\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Analysis\n",
    "\n",
    "Analyze numerical and categorical features separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Numerical columns:\", numerical_cols.tolist())\n",
    "print(\"\\nCategorical columns:\", categorical_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical features\n",
    "for col in categorical_cols[:3]:  # Start with first 3 columns\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_values[missing_values > 0].plot(kind='bar')\n",
    "plt.title('Missing Values by Column')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill numerical missing values with median\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill categorical missing values with mode\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "    return outliers\n",
    "\n",
    "# Check outliers in numerical columns\n",
    "for col in numerical_cols:\n",
    "    outliers = detect_outliers(df, col)\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\nOutliers in {col}: {len(outliers)} values\")\n",
    "        \n",
    "        # Box plot to visualize outliers\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Box Plot of {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Engineering - Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# For binary categorical variables, use Label Encoding\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() == 2:\n",
    "        df[f\"{col}_encoded\"] = le.fit_transform(df[col])\n",
    "\n",
    "# For non-binary categorical variables, use One-Hot Encoding\n",
    "for col in categorical_cols:\n",
    "    if df[col].nunique() > 2:\n",
    "        # Create dummy variables\n",
    "        dummies = pd.get_dummies(df[col], prefix=col)\n",
    "        # Add dummy variables to the dataframe\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        # Remove the original column\n",
    "        df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            # Forward pass\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
